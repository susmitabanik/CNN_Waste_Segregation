{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf5lYawIw8tE"
   },
   "source": [
    "# **Waste Material Segregation for Improving Waste Management**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY1InIbkw80B"
   },
   "source": [
    "## **Objective**\n",
    "\n",
    "The objective of this project is to implement an effective waste material segregation system using convolutional neural networks (CNNs) that categorises waste into distinct groups. This process enhances recycling efficiency, minimises environmental pollution, and promotes sustainable waste management practices.\n",
    "\n",
    "The key goals are:\n",
    "\n",
    "* Accurately classify waste materials into categories like cardboard, glass, paper, and plastic.\n",
    "* Improve waste segregation efficiency to support recycling and reduce landfill waste.\n",
    "* Understand the properties of different waste materials to optimise sorting methods for sustainability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZGTCfyUxalZ"
   },
   "source": [
    "## **Data Understanding**\n",
    "\n",
    "The Dataset consists of images of some common waste materials.\n",
    "\n",
    "1. Food Waste\n",
    "2. Metal\n",
    "3. Paper\n",
    "4. Plastic\n",
    "5. Other\n",
    "6. Cardboard\n",
    "7. Glass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZJtmMnzQjAr"
   },
   "source": [
    "**Data Description**\n",
    "\n",
    "* The dataset consists of multiple folders, each representing a specific class, such as `Cardboard`, `Food_Waste`, and `Metal`.\n",
    "* Within each folder, there are images of objects that belong to that category.\n",
    "* However, these items are not further subcategorised. <br> For instance, the `Food_Waste` folder may contain images of items like coffee grounds, teabags, and fruit peels, without explicitly stating that they are actually coffee grounds or teabags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBFt43WDzWSJ"
   },
   "source": [
    "## **1. Load the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfy0rjJ1yzFl"
   },
   "source": [
    "Load and unzip the dataset zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N35LLuWXzUQH"
   },
   "source": [
    "**Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DmZo7m1-J_Ou"
   },
   "outputs": [],
   "source": [
    "# Recommended versions:\n",
    "\n",
    "# numpy version: 1.26.4\n",
    "# pandas version: 2.2.2\n",
    "# seaborn version: 0.13.2\n",
    "# matplotlib version: 3.10.0\n",
    "# PIL version: 11.1.0\n",
    "# tensorflow version: 2.18.0\n",
    "# keras version: 3.8.0\n",
    "# sklearn version: 1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzM50pygphUe"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import os\n",
    "os.environ['IF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from PIL import Image # For image operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNAzJi1c9WAX"
   },
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TM1qn2DKtjR6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset unzipped successfully to: Dataset_Waste_Segregation\n"
     ]
    }
   ],
   "source": [
    "# Load and unzip the dataset\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zip_file_path = r'Dataset_Waste_Segregation.zip'\n",
    "unzip_destination = r'Dataset_Waste_Segregation'\n",
    "\n",
    "# Unzip the dataset\n",
    "try:\n",
    "    with ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(unzip_destination)\n",
    "    print(f\"Dataset unzipped successfully to: {unzip_destination}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Zip file not found at {zip_file_path}. Please check the path.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: The file at {zip_file_path} is not a valid zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during unzipping: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDp_EWxVOhUu"
   },
   "source": [
    "## **2. Data Preparation** <font color=red> [25 marks] </font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7Ac8VxvjWnw"
   },
   "source": [
    "### **2.1 Load and Preprocess Images** <font color=red> [8 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghmtINrMXDMy"
   },
   "source": [
    "Let us create a function to load the images first. We can then directly use this function while loading images of the different categories to load and crop them in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZQ1UZNfQCWX"
   },
   "source": [
    "#### **2.1.1** <font color=red> [3 marks] </font><br>\n",
    "Create a function to load the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y6klNk9rcAtr"
   },
   "outputs": [],
   "source": [
    "# Create a function to load the raw images\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    try:\n",
    "        img = load_img(image_path, target_size=target_size)\n",
    "\n",
    "        # Convert the PIL Image to a NumPy array\n",
    "        img_array = img_to_array(img)\n",
    "\n",
    "        # Normalize pixel values to [0, 1]\n",
    "        img_array = img_array / 255.0\n",
    "\n",
    "        return img_array\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or preprocessing image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J01VQrLhQsxx"
   },
   "source": [
    "#### **2.1.2** <font color=red> [5 marks] </font><br>\n",
    "Load images and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C9Oo0PTtYLf"
   },
   "source": [
    "Load the images from the dataset directory. Labels of images are present in the subdirectories.\n",
    "\n",
    "Verify if the images and labels are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Zm2zlZbmamzy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading images from: Dataset_Waste_Segregation\n",
      "Processing class: Dataset_Waste_Segregation\n",
      "Error loading or preprocessing image Dataset_Waste_Segregation/Dataset_Waste_Segregation/data.zip: name 'load_img' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m                 all_labels\u001b[38;5;241m.\u001b[39mappend(class_name)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Convert lists to NumPy arrays\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_images)\n\u001b[1;32m     35\u001b[0m y_raw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_labels)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Verifing the images and labels are loaded correctly\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the images and their labels\n",
    "dataset_base_dir = os.path.join(unzip_destination)\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 128, 128\n",
    "TARGET_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "class_names = [] # To store the names of the classes\n",
    "\n",
    "print(f\"\\nLoading images from: {dataset_base_dir}\")\n",
    "\n",
    "# Iterating through each subdirectory\n",
    "for class_name in os.listdir(dataset_base_dir):\n",
    "    class_dir = os.path.join(dataset_base_dir, class_name)\n",
    "\n",
    "    # Checking that its a directory and no files are processeed if present in parent folder.\n",
    "    if os.path.isdir(class_dir):\n",
    "        class_names.append(class_name)\n",
    "        print(f\"Processing class: {class_name}\")\n",
    "\n",
    "        # Iterating through each image file in the class directory\n",
    "        for image_filename in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_filename)\n",
    "\n",
    "            # Load and preprocess the image using our defined function\n",
    "            processed_img = load_and_preprocess_image(image_path, target_size=TARGET_SIZE)\n",
    "\n",
    "            if processed_img is not None:\n",
    "                all_images.append(processed_img)\n",
    "                all_labels.append(class_name)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(all_images)\n",
    "y_raw = np.array(all_labels)\n",
    "\n",
    "# Verifing the images and labels are loaded correctly\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"Total images loaded: {len(all_images)}\")\n",
    "print(f\"Total labels loaded: {len(all_labels)}\")\n",
    "print(f\"Shape of image data (X): {X.shape}\")\n",
    "print(f\"Shape of label data (y_raw): {y_raw.shape}\")\n",
    "print(f\"Detected classes: {class_names}\")\n",
    "\n",
    "if len(class_names) == 0:\n",
    "    print(\"No classes or images found. Please check 'dataset_base_dir' path and dataset structure.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26Is-EwKuyGf"
   },
   "source": [
    "Perform any operations, if needed, on the images and labels to get them into the desired format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I64rs77bkAYk"
   },
   "source": [
    "### **2.2 Data Visualisation** <font color=red> [9 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCAepbyAQdI2"
   },
   "source": [
    "#### **2.2.1** <font color=red> [3 marks] </font><br>\n",
    "Create a bar plot to display the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm5LuWSFqTac"
   },
   "outputs": [],
   "source": [
    "# Visualise Data Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=y_raw, order=class_names)\n",
    "plt.title('Distribution of Waste Categories')\n",
    "plt.xlabel('Waste Category')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNWsPfTzRh7x"
   },
   "source": [
    "#### **2.2.2** <font color=red> [3 marks] </font><br>\n",
    "Visualise some sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37yXZzfLyOWt"
   },
   "outputs": [],
   "source": [
    "# Visualise Sample Images (across different labels)\n",
    "\n",
    "num_samples_per_class = 3\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Get indices of images belonging to the current class\n",
    "    class_indices = np.where(y_raw == class_name)[0]\n",
    "    np.random.shuffle(class_indices)\n",
    "    sample_indices = class_indices[:num_samples_per_class]\n",
    "\n",
    "    for j, idx in enumerate(sample_indices):\n",
    "        plt.subplot(len(class_names), num_samples_per_class, i * num_samples_per_class + j + 1)\n",
    "        plt.imshow(X[idx])\n",
    "        plt.title(f\"{class_name}\")\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrxdFzNigaYG"
   },
   "source": [
    "#### **2.2.3** <font color=red> [3 marks] </font><br>\n",
    "Based on the smallest and largest image dimensions, resize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyVvjNXqgIGe"
   },
   "outputs": [],
   "source": [
    "# Find the smallest and largest image dimensions from the data set\n",
    "original_widths = []\n",
    "original_heights = []\n",
    "\n",
    "# Re-iterate through dataset to get original dimensions (without preprocessing)\n",
    "for class_name in os.listdir(dataset_base_dir):\n",
    "    class_dir = os.path.join(dataset_base_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        for image_filename in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_filename)\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    original_widths.append(img.width)\n",
    "                    original_heights.append(img.height)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not open image {image_path} for dimension analysis: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fz7EutUrgKFZ"
   },
   "outputs": [],
   "source": [
    "# Resize the image dimensions\n",
    "if original_widths and original_heights:\n",
    "    min_width = np.min(original_widths)\n",
    "    max_width = np.max(original_widths)\n",
    "    min_height = np.min(original_heights)\n",
    "    max_height = np.max(original_heights)\n",
    "\n",
    "    print(f\"Smallest original width: {min_width} pixels\")\n",
    "    print(f\"Largest original width: {max_width} pixels\")\n",
    "    print(f\"Smallest original height: {min_height} pixels\")\n",
    "    print(f\"Largest original height: {max_height} pixels\")\n",
    "    print(f\"Chosen target size for CNN: {TARGET_SIZE} (width, height)\")\n",
    "    print(\"Images have been resized to this target size during loading.\")\n",
    "else:\n",
    "    print(\"No images found for dimension analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCB8uOckR5li"
   },
   "source": [
    "### **2.3 Encoding the classes** <font color=red> [3 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdC4dpTWt9eo"
   },
   "source": [
    "There are seven classes present in the data.\n",
    "\n",
    "We have extracted the images and their labels, and visualised their distribution. Now, we need to perform encoding on the labels. Encode the labels suitably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Nwd0Ztvkf7K"
   },
   "source": [
    "####**2.3.1** <font color=red> [3 marks] </font><br>\n",
    "Encode the target class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkyXDQN-660s"
   },
   "outputs": [],
   "source": [
    "# Encode the labels suitably\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "# Convert encoded labels to one-hot encoding\n",
    "num_classes = len(class_names)\n",
    "y_one_hot = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "print(f\"Original labels (first 5): {y_raw[:5]}\")\n",
    "print(f\"Encoded labels (first 5): {y_encoded[:5]}\")\n",
    "print(f\"One-Hot Encoded labels (first 5 rows):\\n{y_one_hot[:5]}\")\n",
    "print(f\"Classes mapping: {list(label_encoder.classes_)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Labels encoded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNBM4hsuSaoj"
   },
   "source": [
    "### **2.4 Data Splitting** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0xw-Qlh29cZ"
   },
   "source": [
    "#### **2.4.1** <font color=red> [5 marks] </font><br>\n",
    "Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TErpx_JOkwjO"
   },
   "outputs": [],
   "source": [
    "# Assign specified parts of the dataset to train and validation sets\n",
    "# Splitting data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "print(f\"Training images shape: {X_train.shape}\")\n",
    "print(f\"Testing images shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")\n",
    "print(\"Dataset split into training and testing sets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mILXPeY-X-zP"
   },
   "source": [
    "## **3. Model Building and Evaluation** <font color=red> [20 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E0afHwy5M_i"
   },
   "source": [
    "### **3.1 Model building and training** <font color=red> [15 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsu8K3tL5a5Q"
   },
   "source": [
    "#### **3.1.1** <font color=red> [10 marks] </font><br>\n",
    "Build and compile the model. Use 3 convolutional layers. Add suitable normalisation, dropout, and fully connected layers to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awW9V2lmMK_d"
   },
   "source": [
    "Test out different configurations and report the results in conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oD7-2EXdz_Cl"
   },
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = Sequential([\n",
    "    # First Convolutional Block\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n",
    "    BatchNormalization(), # Normalize activations\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25), # Regularization to prevent overfitting\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Flatten the output for the fully connected layers\n",
    "    Flatten(),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5), # More aggressive dropout for dense layers\n",
    "    Dense(num_classes, activation='softmax') # Output layer with softmax for multi-class classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t4duT1wX5wS"
   },
   "source": [
    "#### **3.1.2** <font color=red> [5 marks] </font><br>\n",
    "Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcrEzo51Qj6w"
   },
   "source": [
    "Use appropriate metrics and callbacks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7Ut0BicH_I8"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# ModelCheckpoint to save the best model during training\n",
    "checkpoint_filepath = 'best_model.h5' # Save the model with the best validation accuracy\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, # Save the entire model\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True, # Only save when validation accuracy improves\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# EarlyStopping to stop training if validation accuracy doesn't improve\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10, # Number of epochs with no improvement after which training will be stopped.\n",
    "    mode='max',\n",
    "    restore_best_weights=True, # Restore model weights from the epoch with the best value of the monitored quantity.\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 20 # Started with higher and going lower due to memory issue.\n",
    "BATCH_SIZE = 18\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test), # Use the test set as validation data\n",
    "    callbacks=[model_checkpoint_callback, early_stopping_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nModel training complete.\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWT-bIj9YVzh"
   },
   "source": [
    "### **3.2 Model Testing and Evaluation** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjhU3i5v59d6"
   },
   "source": [
    "#### **3.2.1** <font color=red> [5 marks] </font><br>\n",
    "Evaluate the model on test dataset. Derive appropriate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_MtfUM_4y7j"
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test set; display suitable metrics\n",
    "# Loading the best model saved by ModelCheckpoint\n",
    "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Evaluating the best model on the test set\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert one-hot encoded true labels back to single integer labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Generate Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "print(\"Model evaluation complete. Metrics displayed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utro5JdHS0JM"
   },
   "source": [
    "## **4. Data Augmentation** <font color=red> [optional] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T6QlG4eS4xi"
   },
   "source": [
    "#### **4.1 Create a Data Augmentation Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AXlfuoa4jQV"
   },
   "source": [
    "##### **4.1.1**\n",
    "Define augmentation steps for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbHCwkX0dq0R"
   },
   "outputs": [],
   "source": [
    "# Define augmentation steps to augment images\n",
    "# Different augmentation techniques for image classification\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,        # Rotate images by a random angle (0-20 degrees)\n",
    "    width_shift_range=0.1,    # Shift images horizontally by 10% of total width\n",
    "    height_shift_range=0.1,   # Shift images vertically by 10% of total height\n",
    "    shear_range=0.1,          # Apply shear transformation\n",
    "    zoom_range=0.1,           # Zoom in/out by 10%\n",
    "    horizontal_flip=True,     # Randomly flip images horizontally\n",
    "    fill_mode='nearest'       # Strategy for filling in newly created pixels\n",
    ")\n",
    "\n",
    "\n",
    "datagen.fit(X_train)\n",
    "print(\"Data augmentation pipeline defined and fitted on training data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07i11vgMEmM2"
   },
   "source": [
    "Augment and resample the images.\n",
    "In case of class imbalance, you can also perform adequate undersampling on the majority class and augment those images to ensure consistency in the input datasets for both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chvmgE2r4xPZ"
   },
   "source": [
    "Augment the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-JBheeYFS8d"
   },
   "outputs": [],
   "source": [
    "# Create a function to augment the images\n",
    "\n",
    "\n",
    "print(\"\\nCreating augmented training dataset generator.\")\n",
    "augmented_train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "print(\"Augmented training dataset created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ddy1y1nPIlvM"
   },
   "outputs": [],
   "source": [
    "# Create the augmented training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZYekkw9TCvP"
   },
   "source": [
    "##### **4.1.2**\n",
    "\n",
    "Train the model on the new augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBcRbt57FEct"
   },
   "outputs": [],
   "source": [
    "# Train the model using augmented images\n",
    "augmented_model = Sequential([\n",
    "    # First Convolutional Block\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Flatten the output for the fully connected layers\n",
    "    Flatten(),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "augmented_model.compile(optimizer='adam',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "augmented_model.summary()\n",
    "print(\"New model initialized and compiled for augmented training.\")\n",
    "\n",
    "# Define callbacks for augmented training\n",
    "checkpoint_filepath_aug = 'best_augmented_model.h5'\n",
    "model_checkpoint_callback_aug = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_aug,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback_aug = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=15, # Increased patience for augmented data as it might take longer to converge\n",
    "    mode='max',\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model using the augmented data generator\n",
    "# Use validation_data for evaluation on the test set\n",
    "history_aug = augmented_model.fit(\n",
    "    augmented_train_generator,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE, # Number of batches per epoch\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[model_checkpoint_callback_aug, early_stopping_callback_aug],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nModel training with augmented data complete.\")\n",
    "\n",
    "# Plot training history for augmented model\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_aug.history['accuracy'])\n",
    "plt.plot(history_aug.history['val_accuracy'])\n",
    "plt.title('Augmented Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_aug.history['loss'])\n",
    "plt.plot(history_aug.history['val_loss'])\n",
    "plt.title('Augmented Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the augmented model\n",
    "print(\"\\n--- Evaluating Augmented Model on Test Set ---\")\n",
    "best_augmented_model = tf.keras.models.load_model(checkpoint_filepath_aug)\n",
    "loss_aug, accuracy_aug = best_augmented_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Augmented Model Test Loss: {loss_aug:.4f}\")\n",
    "print(f\"Augmented Model Test Accuracy: {accuracy_aug:.4f}\")\n",
    "\n",
    "y_pred_probs_aug = best_augmented_model.predict(X_test)\n",
    "y_pred_aug = np.argmax(y_pred_probs_aug, axis=1)\n",
    "print(\"\\nClassification Report (Augmented Model):\")\n",
    "print(classification_report(y_true, y_pred_aug, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFPuXAvHkJVz"
   },
   "source": [
    "## **5. Conclusions** <font color = red> [5 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33tWCHjpO5hH"
   },
   "source": [
    "#### **5.1 Conclude with outcomes and insights gained** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3e1TLo2kWi0"
   },
   "source": [
    "* Report your findings about the data\n",
    "* Report model training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many materials which are of same type and create issue in the classification.\n",
    "There are many items which if tried to be caught then creates a lot of noise.\n",
    "High iteration and sample sizes are good for getting best resutls but should be optimized due to resource contraints.\n",
    "It is best to go for iterative learning to find the best segregation of the garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1KW2JSuqBLb3DdmqZSAHtN2K0gX8C2HcV",
     "timestamp": 1740722968634
    },
    {
     "file_id": "1XXsgvgvRpr1OqI_K70kBWgfsI9bByK3r",
     "timestamp": 1738303842187
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
